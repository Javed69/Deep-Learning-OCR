# Deep-Learning-OCR
We will create a simple neural network, known as a perceptron, to classify handwritten digits into ‘five’ or ‘not five’. This is known as optical character recognition. MNIST: Modified National Institute of Standards and Technology . Perceptrons take in an array of numbers - in this case pixel values - and output a single number. The output will be a 1 if the digit is a five, and 0 if it is any other number. We train perceptrons with training data, which are the handwritten digits labelled with what number they are. ‍ Perceptrons weight each of the input numbers to reach a correct classification. For instance, if we start with random weights and train our model with some data that it knows is a five, it will adjust how much it weights each of pixels depending on how predictive they are. This may mess up the weights for the earlier data but we don't worry about that now. Running through all of the data once and adjusting these weights is known as completing one epoch. After one epoch, we will have an initial guess at the weights for each pixel. We can then run through all of the data again and again, fine tuning the weights each time. Finally, we take the weighted sum of the pixel values to create an output number. We can interpret the output however we want: in this case anything greater than 0 means that it is a 5. Understanding the code How do we transfer our intuition to code? We use the popular Machine Learning library Keras to create our model in just 30 lines of code. We feed this model some training data, but hold out a portion as validation data. This is used to test our model later, to make sure that the model hasn’t just learnt the training data (known as overfitting) and can generalize to new data too. ‍ In order to find the model with the best weights, keras uses backpropagation. Backpropagation means that when the model adjusts the weights based on new inputs, it calculates how it would have affected the previous data that it has seen. This is the intuition for gradient descent. There are different ways to calculate loss, known as loss functions. In this section we use Mean Squared Error (MSE). Another parameter we define in our model is the learning rate. This determines how fast we adjust (or ‘learn’) weights in response to seeing new data. If this rate is too slow, it will take too long to learn the optimal weights. However, if the rate is too fast we may skip over the optimal weights. The best choice of learning rate depends on the exact problem, and is a focus of machine learning research.
